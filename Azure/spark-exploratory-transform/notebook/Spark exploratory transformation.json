{
	"name": "Spark exploratory transformation",
	"properties": {
		"nbformat": 0,
		"nbformat_minor": 0,
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Using Azure Open Datasets in Synapse - Enrich NYC Green Taxi Data with Holiday and Weather"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.opendatasets import NycTlcGreen\n",
					"\n",
					"from datetime import datetime\n",
					"from dateutil import parser\n",
					"end_date = parser.parse('2018-06-12')\n",
					"start_date = parser.parse('2018-06-01')\n",
					"\n",
					"nyc_tlc = NycTlcGreen(start_date=start_date, end_date=end_date)\n",
					"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
				],
				"attachments": null,
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Perform data exploratory analysis"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Schema of the dataset\n",
					"\n",
					"nyc_tlc_df.printSchema()\n",
					"\n",
					"# Display 5 rows\n",
					"\n",
					"nyc_tlc_df.show(5, truncate = False)\n",
					"\n",
					"# Display number of the record in the dataframe\n",
					"\n",
					"nyc_tlc_df.count()\n",
					"\n",
					"\n",
					"\n",
					"# Statistical properties of a field\n",
					"\n",
					"nyc_tlc_df.describe([\"fareAmount\"]).show()\n",
					"\n",
					"\n",
					"# Remove unused columns from nyc green taxi data\n",
					"# To remove any column from the pyspark dataframe, use the drop function. Example df = df.drop(“col_name”)\n",
					"\n",
					"columns_to_remove = [\"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\",  \n",
					"                     \"pickupLatitude\", \"dropoffLongitude\",\"dropoffLatitude\" ,\"rateCodeID\", \n",
					"                     \"storeAndFwdFlag\",\"paymentType\", \"fareAmount\", \"extra\", \"mtaTax\",\n",
					"                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType \"  \n",
					"                    ]\n",
					"\n",
					"nyc_tlc_df_clean = nyc_tlc_df.select([column for column in nyc_tlc_df.columns if column not in columns_to_remove])\n",
					"\n",
					"# Display 5 rows\n",
					"nyc_tlc_df_clean.show(5)\n",
					"\n",
					"# Find unique values of a categorical column\n",
					"\n",
					"nyc_tlc_df_clean.select('vendorID').distinct()\n",
					"\n",
					"\n",
					"# Count the missing values of PySpark Dataframe column\n",
					"# To know the missing values, we first count the null values in a dataframe.\n",
					"nyc_tlc_df_clean.filter(nyc_tlc_df_clean['pickupLongitude'].isNull()).count()"
				],
				"attachments": null,
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Tranformation of Spark dataframe "
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. \n",
					"\n",
					"import pyspark.sql.functions as f\n",
					"\n",
					"nyc_tlc_df_expand = nyc_tlc_df.withColumn('datetime',f.to_date('lpepPickupDatetime'))\\\n",
					"                .withColumn('month_num',f.month(nyc_tlc_df.lpepPickupDatetime))\\\n",
					"                .withColumn('day_of_month',f.dayofmonth(nyc_tlc_df.lpepPickupDatetime))\\\n",
					"                .withColumn('day_of_week',f.dayofweek(nyc_tlc_df.lpepPickupDatetime))\\\n",
					"                .withColumn('hour_of_day',f.hour(nyc_tlc_df.lpepPickupDatetime))\\\n",
					"                .withColumn('country_code',f.lit('US'))"
				],
				"attachments": null,
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"Remove some of the columns that won't need for modeling or additional feature building.\n",
					"\n",
					"\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Enrich with holiday data"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.opendatasets import PublicHolidays\n",
					"\n",
					"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
					"hol_df = hol.to_spark_dataframe()\n",
					"\n",
					"# Display data\n",
					"hol_df.show(5, truncate = False)"
				],
				"attachments": null,
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"Rename the countryRegionCode and date columns to match the respective field names from the taxi data, and also normalize the time so it can be used as a key. "
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"hol_df_clean = hol_df.withColumnRenamed('countryRegionCode','country_code')\\\n",
					"            .withColumn('datetime',f.to_date('date'))\n",
					"\n",
					"hol_df_clean.show(5)"
				],
				"attachments": null,
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"Next, join the holiday data with the taxi data by performing a left-join. This will preserve all records from taxi data, but add in holiday data where it exists for the corresponding datetime and country_code, which in this case is always \"US\". Preview the data to verify that they were merged correctly."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# enrich taxi data with holiday data\n",
					"nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, on = ['datetime', 'country_code'] , how = 'left')\n",
					"\n",
					"nyc_taxi_holiday_df.show(5)"
				],
				"attachments": null,
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"# Create a temp table and filter out non empty holiday rows\n",
					"\n",
					"nyc_taxi_holiday_df.createOrReplaceTempView(\"nyc_taxi_holiday_df\")\n",
					"spark.sql(\"SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL \").show(5, truncate = False)"
				],
				"attachments": null,
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Enrich with weather data¶\n",
					"\n",
					"Now we append NOAA surface weather data to the taxi and holiday data. Use a similar approach to fetch the [NOAA weather history data](https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/) from Azure Open Datasets. "
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.opendatasets import NoaaIsdWeather\n",
					"\n",
					"isd = NoaaIsdWeather(start_date, end_date)\n",
					"isd_df = isd.to_spark_dataframe()"
				],
				"attachments": null,
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"isd_df.show(5, truncate = False)"
				],
				"attachments": null,
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"# Filter out weather info for new york city, remove the recording with null temperature \n",
					"\n",
					"weather_df = isd_df.filter(isd_df.latitude >= '40.53')\\\n",
					"                        .filter(isd_df.latitude <= '40.88')\\\n",
					"                        .filter(isd_df.longitude >= '-74.09')\\\n",
					"                        .filter(isd_df.longitude <= '-73.72')\\\n",
					"                        .filter(isd_df.temperature.isNotNull())\\\n",
					"                        .withColumnRenamed('datetime','datetime_full')\n",
					"                         "
				],
				"attachments": null,
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"# Remove unused columns\n",
					"\n",
					"columns_to_remove_weather = [\"usaf\", \"wban\", \"longitude\", \"latitude\"]\n",
					"weather_df_clean = weather_df.select([column for column in weather_df.columns if column not in columns_to_remove_weather])\\\n",
					"                        .withColumn('datetime',f.to_date('datetime_full'))\n",
					"\n",
					"weather_df_clean.show(5, truncate = False)"
				],
				"attachments": null,
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"source": [
					"Next group the weather data so that you have daily aggregated weather values. \n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Enrich weather data with aggregation statistics\n",
					"\n",
					"aggregations = {\"snowDepth\": \"mean\", \"precipTime\": \"max\", \"temperature\": \"mean\", \"precipDepth\": \"max\"}\n",
					"weather_df_grouped = weather_df_clean.groupby(\"datetime\").agg(aggregations)"
				],
				"attachments": null,
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"source": [
					"weather_df_grouped.show(5)"
				],
				"attachments": null,
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"# Rename columns\n",
					"\n",
					"weather_df_grouped = weather_df_grouped.withColumnRenamed('avg(snowDepth)','avg_snowDepth')\\\n",
					"                                       .withColumnRenamed('avg(temperature)','avg_temperature')\\\n",
					"                                       .withColumnRenamed('max(precipTime)','max_precipTime')\\\n",
					"                                       .withColumnRenamed('max(precipDepth)','max_precipDepth')"
				],
				"attachments": null,
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"source": [
					"Merge the taxi and holiday data you prepared with the new weather data. This time you only need the datetime key, and again perform a left-join of the data. Run the describe() function on the new dataframe to see summary statistics for each field."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# enrich taxi data with weather\n",
					"nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, on = 'datetime' , how = 'left')\n",
					"nyc_taxi_holiday_weather_df.cache()"
				],
				"attachments": null,
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					"nyc_taxi_holiday_weather_df.show(5)"
				],
				"attachments": null,
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# Run the describe() function on the new dataframe to see summary statistics for each field.\n",
					"\n",
					"display(nyc_taxi_holiday_weather_df.describe())"
				],
				"attachments": null,
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"source": [
					"The summary statistics shows that the totalAmount field has negative values, which don't make sense in the context.\n",
					"\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Remove invalid rows with less than 0 taxi fare or tip\n",
					"final_df = nyc_taxi_holiday_weather_df.filter(nyc_taxi_holiday_weather_df.tipAmount > 0)\\\n",
					"                                      .filter(nyc_taxi_holiday_weather_df.totalAmount > 0)"
				],
				"attachments": null,
				"execution_count": 20
			}
		]
	}
}