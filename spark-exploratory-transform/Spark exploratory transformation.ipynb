{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Azure Open Datasets in Synapse - Enrich NYC Green Taxi Data with Holiday and Weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import NycTlcGreen\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "end_date = parser.parse('2018-06-12')\n",
        "start_date = parser.parse('2018-06-01')\n",
        "\n",
        "nyc_tlc = NycTlcGreen(start_date=start_date, end_date=end_date)\n",
        "nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Perform data exploratory analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Schema of the dataset\n",
        "\n",
        "nyc_tlc_df.printSchema()\n",
        "\n",
        "# Display 5 rows\n",
        "\n",
        "nyc_tlc_df.show(5, truncate = False)\n",
        "\n",
        "# Display number of the record in the dataframe\n",
        "\n",
        "nyc_tlc_df.count()\n",
        "\n",
        "\n",
        "\n",
        "# Statistical properties of a field\n",
        "\n",
        "nyc_tlc_df.describe([\"fareAmount\"]).show()\n",
        "\n",
        "\n",
        "# Remove unused columns from nyc green taxi data\n",
        "# To remove any column from the pyspark dataframe, use the drop function. Example df = df.drop(“col_name”)\n",
        "\n",
        "columns_to_remove = [\"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\",  \n",
        "                     \"pickupLatitude\", \"dropoffLongitude\",\"dropoffLatitude\" ,\"rateCodeID\", \n",
        "                     \"storeAndFwdFlag\",\"paymentType\", \"fareAmount\", \"extra\", \"mtaTax\",\n",
        "                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType \"  \n",
        "                    ]\n",
        "\n",
        "nyc_tlc_df_clean = nyc_tlc_df.select([column for column in nyc_tlc_df.columns if column not in columns_to_remove])\n",
        "\n",
        "# Display 5 rows\n",
        "nyc_tlc_df_clean.show(5)\n",
        "\n",
        "# Find unique values of a categorical column\n",
        "\n",
        "nyc_tlc_df_clean.select('vendorID').distinct()\n",
        "\n",
        "\n",
        "# Count the missing values of PySpark Dataframe column\n",
        "# To know the missing values, we first count the null values in a dataframe.\n",
        "nyc_tlc_df_clean.filter(nyc_tlc_df_clean['pickupLongitude'].isNull()).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tranformation of Spark dataframe "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. \n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "nyc_tlc_df_expand = nyc_tlc_df.withColumn('datetime',f.to_date('lpepPickupDatetime'))\\\n",
        "                .withColumn('month_num',f.month(nyc_tlc_df.lpepPickupDatetime))\\\n",
        "                .withColumn('day_of_month',f.dayofmonth(nyc_tlc_df.lpepPickupDatetime))\\\n",
        "                .withColumn('day_of_week',f.dayofweek(nyc_tlc_df.lpepPickupDatetime))\\\n",
        "                .withColumn('hour_of_day',f.hour(nyc_tlc_df.lpepPickupDatetime))\\\n",
        "                .withColumn('country_code',f.lit('US'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove some of the columns that won't need for modeling or additional feature building.\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enrich with holiday data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import PublicHolidays\n",
        "\n",
        "hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
        "hol_df = hol.to_spark_dataframe()\n",
        "\n",
        "# Display data\n",
        "hol_df.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rename the countryRegionCode and date columns to match the respective field names from the taxi data, and also normalize the time so it can be used as a key. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "hol_df_clean = hol_df.withColumnRenamed('countryRegionCode','country_code')\\\n",
        "            .withColumn('datetime',f.to_date('date'))\n",
        "\n",
        "hol_df_clean.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, join the holiday data with the taxi data by performing a left-join. This will preserve all records from taxi data, but add in holiday data where it exists for the corresponding datetime and country_code, which in this case is always \"US\". Preview the data to verify that they were merged correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "# enrich taxi data with holiday data\n",
        "nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, on = ['datetime', 'country_code'] , how = 'left')\n",
        "\n",
        "nyc_taxi_holiday_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create a temp table and filter out non empty holiday rows\n",
        "\n",
        "nyc_taxi_holiday_df.createOrReplaceTempView(\"nyc_taxi_holiday_df\")\n",
        "spark.sql(\"SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL \").show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enrich with weather data¶\n",
        "\n",
        "Now we append NOAA surface weather data to the taxi and holiday data. Use a similar approach to fetch the [NOAA weather history data](https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/) from Azure Open Datasets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import NoaaIsdWeather\n",
        "\n",
        "isd = NoaaIsdWeather(start_date, end_date)\n",
        "isd_df = isd.to_spark_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "isd_df.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Filter out weather info for new york city, remove the recording with null temperature \n",
        "\n",
        "weather_df = isd_df.filter(isd_df.latitude >= '40.53')\\\n",
        "                        .filter(isd_df.latitude <= '40.88')\\\n",
        "                        .filter(isd_df.longitude >= '-74.09')\\\n",
        "                        .filter(isd_df.longitude <= '-73.72')\\\n",
        "                        .filter(isd_df.temperature.isNotNull())\\\n",
        "                        .withColumnRenamed('datetime','datetime_full')\n",
        "                         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Remove unused columns\n",
        "\n",
        "columns_to_remove_weather = [\"usaf\", \"wban\", \"longitude\", \"latitude\"]\n",
        "weather_df_clean = weather_df.select([column for column in weather_df.columns if column not in columns_to_remove_weather])\\\n",
        "                        .withColumn('datetime',f.to_date('datetime_full'))\n",
        "\n",
        "weather_df_clean.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next group the weather data so that you have daily aggregated weather values. \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Enrich weather data with aggregation statistics\n",
        "\n",
        "aggregations = {\"snowDepth\": \"mean\", \"precipTime\": \"max\", \"temperature\": \"mean\", \"precipDepth\": \"max\"}\n",
        "weather_df_grouped = weather_df_clean.groupby(\"datetime\").agg(aggregations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "weather_df_grouped.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Rename columns\n",
        "\n",
        "weather_df_grouped = weather_df_grouped.withColumnRenamed('avg(snowDepth)','avg_snowDepth')\\\n",
        "                                       .withColumnRenamed('avg(temperature)','avg_temperature')\\\n",
        "                                       .withColumnRenamed('max(precipTime)','max_precipTime')\\\n",
        "                                       .withColumnRenamed('max(precipDepth)','max_precipDepth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge the taxi and holiday data you prepared with the new weather data. This time you only need the datetime key, and again perform a left-join of the data. Run the describe() function on the new dataframe to see summary statistics for each field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "# enrich taxi data with weather\n",
        "nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, on = 'datetime' , how = 'left')\n",
        "nyc_taxi_holiday_weather_df.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "nyc_taxi_holiday_weather_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Run the describe() function on the new dataframe to see summary statistics for each field.\n",
        "\n",
        "display(nyc_taxi_holiday_weather_df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The summary statistics shows that the totalAmount field has negative values, which don't make sense in the context.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Remove invalid rows with less than 0 taxi fare or tip\n",
        "final_df = nyc_taxi_holiday_weather_df.filter(nyc_taxi_holiday_weather_df.tipAmount > 0)\\\n",
        "                                      .filter(nyc_taxi_holiday_weather_df.totalAmount > 0)"
      ]
    }
  ]
}